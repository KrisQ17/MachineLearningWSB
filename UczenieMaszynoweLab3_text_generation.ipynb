{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Trt671D32ewv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4_Pz2kv_2jkX"
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3BIOrN3t2nCo"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A5tn6PRk2pl0"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZj5wLsg2rAu",
    "outputId": "5f0cbe7f-49b3-4b34-f07c-6a46d332d768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0 65536    0     0  35890      0  0:39:03  0:00:01  0:39:02 35870\n",
      "  0 80.2M    0  272k    0     0  97183      0  0:14:25  0:00:02  0:14:23 97149\n",
      "  0 80.2M    0  560k    0     0   150k      0  0:09:07  0:00:03  0:09:04  150k\n",
      "  1 80.2M    1  928k    0     0   194k      0  0:07:02  0:00:04  0:06:58  194k\n",
      "  1 80.2M    1 1312k    0     0   226k      0  0:06:03  0:00:05  0:05:58  261k\n",
      "  2 80.2M    2 1680k    0     0   245k      0  0:05:34  0:00:06  0:05:28  322k\n",
      "  2 80.2M    2 2064k    0     0   262k      0  0:05:13  0:00:07  0:05:06  357k\n",
      "  2 80.2M    2 2384k    0     0   272k      0  0:05:01  0:00:08  0:04:53  363k\n",
      "  3 80.2M    3 2768k    0     0   283k      0  0:04:50  0:00:09  0:04:41  367k\n",
      "  3 80.2M    3 3152k    0     0   291k      0  0:04:42  0:00:10  0:04:32  366k\n",
      "  4 80.2M    4 3536k    0     0   298k      0  0:04:35  0:00:11  0:04:24  369k\n",
      "  4 80.2M    4 3856k    0     0   303k      0  0:04:30  0:00:12  0:04:18  369k\n",
      "  5 80.2M    5 4240k    0     0   308k      0  0:04:26  0:00:13  0:04:13  370k\n",
      "  5 80.2M    5 4624k    0     0   312k      0  0:04:22  0:00:14  0:04:08  371k\n",
      "  6 80.2M    6 4992k    0     0   315k      0  0:04:20  0:00:15  0:04:05  368k\n",
      "  6 80.2M    6 5376k    0     0   319k      0  0:04:17  0:00:16  0:04:01  368k\n",
      "  6 80.2M    6 5728k    0     0   323k      0  0:04:14  0:00:17  0:03:57  374k\n",
      "  7 80.2M    7 6096k    0     0   325k      0  0:04:12  0:00:18  0:03:54  371k\n",
      "  7 80.2M    7 6464k    0     0   326k      0  0:04:11  0:00:19  0:03:52  367k\n",
      "  8 80.2M    8 6848k    0     0   328k      0  0:04:09  0:00:20  0:03:49  370k\n",
      "  8 80.2M    8 7232k    0     0   330k      0  0:04:08  0:00:21  0:03:47  370k\n",
      "  9 80.2M    9 7552k    0     0   332k      0  0:04:07  0:00:22  0:03:45  363k\n",
      "  9 80.2M    9 7920k    0     0   333k      0  0:04:06  0:00:23  0:03:43  363k\n",
      " 10 80.2M   10 8240k    0     0   332k      0  0:04:07  0:00:24  0:03:43  354k\n",
      " 10 80.2M   10 8544k    0     0   330k      0  0:04:08  0:00:25  0:03:43  338k\n",
      " 10 80.2M   10 8912k    0     0   331k      0  0:04:07  0:00:26  0:03:41  335k\n",
      " 11 80.2M   11 9232k    0     0   332k      0  0:04:06  0:00:27  0:03:39  335k\n",
      " 11 80.2M   11 9616k    0     0   334k      0  0:04:05  0:00:28  0:03:37  338k\n",
      " 12 80.2M   12 9984k    0     0   334k      0  0:04:05  0:00:29  0:03:36  347k\n",
      " 12 80.2M   12 10.1M    0     0   336k      0  0:04:04  0:00:30  0:03:34  366k\n",
      " 13 80.2M   13 10.4M    0     0   337k      0  0:04:03  0:00:31  0:03:32  368k\n",
      " 13 80.2M   13 10.8M    0     0   338k      0  0:04:02  0:00:32  0:03:30  367k\n",
      " 13 80.2M   13 11.1M    0     0   339k      0  0:04:02  0:00:33  0:03:29  366k\n",
      " 14 80.2M   14 11.5M    0     0   339k      0  0:04:01  0:00:34  0:03:27  370k\n",
      " 14 80.2M   14 11.8M    0     0   340k      0  0:04:01  0:00:35  0:03:26  367k\n",
      " 15 80.2M   15 12.2M    0     0   341k      0  0:04:00  0:00:36  0:03:24  365k\n",
      " 15 80.2M   15 12.6M    0     0   342k      0  0:04:00  0:00:37  0:03:23  367k\n",
      " 16 80.2M   16 13.0M    0     0   342k      0  0:03:59  0:00:38  0:03:21  368k\n",
      " 16 80.2M   16 13.2M    0     0   342k      0  0:03:59  0:00:39  0:03:20  363k\n",
      " 17 80.2M   17 13.6M    0     0   343k      0  0:03:59  0:00:40  0:03:19  362k\n",
      " 17 80.2M   17 14.0M    0     0   344k      0  0:03:58  0:00:41  0:03:17  365k\n",
      " 17 80.2M   17 14.4M    0     0   344k      0  0:03:58  0:00:42  0:03:16  366k\n",
      " 18 80.2M   18 14.7M    0     0   345k      0  0:03:57  0:00:43  0:03:14  366k\n",
      " 18 80.2M   18 15.1M    0     0   346k      0  0:03:57  0:00:44  0:03:13  374k\n",
      " 19 80.2M   19 15.4M    0     0   346k      0  0:03:57  0:00:45  0:03:12  371k\n",
      " 19 80.2M   19 15.8M    0     0   347k      0  0:03:56  0:00:46  0:03:10  372k\n",
      " 20 80.2M   20 16.2M    0     0   347k      0  0:03:56  0:00:47  0:03:09  370k\n",
      " 20 80.2M   20 16.5M    0     0   347k      0  0:03:56  0:00:48  0:03:08  367k\n",
      " 21 80.2M   21 16.9M    0     0   348k      0  0:03:55  0:00:49  0:03:06  363k\n",
      " 21 80.2M   21 17.2M    0     0   348k      0  0:03:55  0:00:50  0:03:05  366k\n",
      " 22 80.2M   22 17.6M    0     0   349k      0  0:03:54  0:00:51  0:03:03  372k\n",
      " 22 80.2M   22 18.0M    0     0   349k      0  0:03:55  0:00:52  0:03:03  368k\n",
      " 22 80.2M   22 18.3M    0     0   349k      0  0:03:54  0:00:53  0:03:01  370k\n",
      " 23 80.2M   23 18.7M    0     0   350k      0  0:03:54  0:00:54  0:03:00  370k\n",
      " 23 80.2M   23 19.0M    0     0   350k      0  0:03:54  0:00:55  0:02:59  370k\n",
      " 24 80.2M   24 19.4M    0     0   351k      0  0:03:53  0:00:56  0:02:57  367k\n",
      " 24 80.2M   24 19.7M    0     0   351k      0  0:03:53  0:00:57  0:02:56  368k\n",
      " 25 80.2M   25 20.1M    0     0   351k      0  0:03:53  0:00:58  0:02:55  369k\n",
      " 25 80.2M   25 20.5M    0     0   351k      0  0:03:53  0:00:59  0:02:54  368k\n",
      " 26 80.2M   26 20.8M    0     0   352k      0  0:03:53  0:01:00  0:02:53  368k\n",
      " 26 80.2M   26 21.2M    0     0   352k      0  0:03:53  0:01:01  0:02:52  365k\n",
      " 26 80.2M   26 21.5M    0     0   352k      0  0:03:53  0:01:02  0:02:51  367k\n",
      " 27 80.2M   27 21.9M    0     0   352k      0  0:03:52  0:01:03  0:02:49  366k\n",
      " 27 80.2M   27 22.3M    0     0   352k      0  0:03:52  0:01:04  0:02:48  367k\n",
      " 28 80.2M   28 22.6M    0     0   353k      0  0:03:52  0:01:05  0:02:47  366k\n",
      " 28 80.2M   28 23.0M    0     0   353k      0  0:03:52  0:01:06  0:02:46  374k\n",
      " 29 80.2M   29 23.3M    0     0   353k      0  0:03:52  0:01:07  0:02:45  368k\n",
      " 29 80.2M   29 23.7M    0     0   353k      0  0:03:52  0:01:08  0:02:44  369k\n",
      " 30 80.2M   30 24.1M    0     0   354k      0  0:03:51  0:01:09  0:02:42  369k\n",
      " 30 80.2M   30 24.5M    0     0   354k      0  0:03:51  0:01:10  0:02:41  369k\n",
      " 30 80.2M   30 24.8M    0     0   354k      0  0:03:51  0:01:11  0:02:40  368k\n",
      " 31 80.2M   31 25.1M    0     0   354k      0  0:03:51  0:01:12  0:02:39  366k\n",
      " 31 80.2M   31 25.5M    0     0   354k      0  0:03:51  0:01:13  0:02:38  365k\n",
      " 32 80.2M   32 25.9M    0     0   354k      0  0:03:51  0:01:14  0:02:37  365k\n",
      " 32 80.2M   32 26.2M    0     0   355k      0  0:03:51  0:01:15  0:02:36  367k\n",
      " 33 80.2M   33 26.6M    0     0   355k      0  0:03:50  0:01:16  0:02:34  370k\n",
      " 33 80.2M   33 27.0M    0     0   355k      0  0:03:50  0:01:17  0:02:33  371k\n",
      " 34 80.2M   34 27.3M    0     0   355k      0  0:03:50  0:01:18  0:02:32  372k\n",
      " 34 80.2M   34 27.7M    0     0   356k      0  0:03:50  0:01:19  0:02:31  372k\n",
      " 35 80.2M   35 28.1M    0     0   356k      0  0:03:50  0:01:20  0:02:30  372k\n",
      " 35 80.2M   35 28.4M    0     0   356k      0  0:03:50  0:01:21  0:02:29  367k\n",
      " 35 80.2M   35 28.7M    0     0   355k      0  0:03:50  0:01:22  0:02:28  357k\n",
      " 36 80.2M   36 29.1M    0     0   356k      0  0:03:50  0:01:23  0:02:27  358k\n",
      " 36 80.2M   36 29.5M    0     0   356k      0  0:03:50  0:01:24  0:02:26  358k\n",
      " 37 80.2M   37 29.8M    0     0   356k      0  0:03:50  0:01:25  0:02:25  358k\n",
      " 37 80.2M   37 30.2M    0     0   356k      0  0:03:50  0:01:26  0:02:24  356k\n",
      " 38 80.2M   38 30.5M    0     0   356k      0  0:03:50  0:01:27  0:02:23  368k\n",
      " 38 80.2M   38 30.9M    0     0   356k      0  0:03:50  0:01:28  0:02:22  368k\n",
      " 39 80.2M   39 31.2M    0     0   356k      0  0:03:50  0:01:29  0:02:21  368k\n",
      " 39 80.2M   39 31.6M    0     0   357k      0  0:03:50  0:01:30  0:02:20  370k\n",
      " 39 80.2M   39 32.0M    0     0   357k      0  0:03:49  0:01:31  0:02:18  370k\n",
      " 40 80.2M   40 32.3M    0     0   357k      0  0:03:49  0:01:32  0:02:17  371k\n",
      " 40 80.2M   40 32.7M    0     0   357k      0  0:03:49  0:01:33  0:02:16  370k\n",
      " 41 80.2M   41 33.1M    0     0   357k      0  0:03:49  0:01:34  0:02:15  370k\n",
      " 41 80.2M   41 33.4M    0     0   357k      0  0:03:49  0:01:35  0:02:14  370k\n",
      " 42 80.2M   42 33.8M    0     0   357k      0  0:03:49  0:01:36  0:02:13  367k\n",
      " 42 80.2M   42 34.1M    0     0   357k      0  0:03:49  0:01:37  0:02:12  367k\n",
      " 43 80.2M   43 34.5M    0     0   357k      0  0:03:49  0:01:38  0:02:11  367k\n",
      " 43 80.2M   43 34.9M    0     0   358k      0  0:03:49  0:01:39  0:02:10  375k\n",
      " 43 80.2M   43 35.2M    0     0   358k      0  0:03:49  0:01:40  0:02:09  368k\n",
      " 44 80.2M   44 35.5M    0     0   358k      0  0:03:49  0:01:41  0:02:08  365k\n",
      " 44 80.2M   44 35.9M    0     0   358k      0  0:03:49  0:01:42  0:02:07  371k\n",
      " 45 80.2M   45 36.3M    0     0   358k      0  0:03:49  0:01:43  0:02:06  367k\n",
      " 45 80.2M   45 36.7M    0     0   358k      0  0:03:48  0:01:44  0:02:04  367k\n",
      " 46 80.2M   46 37.0M    0     0   358k      0  0:03:48  0:01:45  0:02:03  367k\n",
      " 46 80.2M   46 37.3M    0     0   358k      0  0:03:48  0:01:46  0:02:02  368k\n",
      " 47 80.2M   47 37.7M    0     0   358k      0  0:03:48  0:01:47  0:02:01  366k\n",
      " 47 80.2M   47 38.1M    0     0   358k      0  0:03:48  0:01:48  0:02:00  370k\n",
      " 47 80.2M   47 38.5M    0     0   359k      0  0:03:48  0:01:49  0:01:59  367k\n",
      " 48 80.2M   48 38.8M    0     0   359k      0  0:03:48  0:01:50  0:01:58  371k\n",
      " 48 80.2M   48 39.1M    0     0   359k      0  0:03:48  0:01:51  0:01:57  367k\n",
      " 49 80.2M   49 39.5M    0     0   359k      0  0:03:48  0:01:52  0:01:56  368k\n",
      " 49 80.2M   49 39.9M    0     0   359k      0  0:03:48  0:01:53  0:01:55  373k\n",
      " 50 80.2M   50 40.3M    0     0   359k      0  0:03:48  0:01:54  0:01:54  372k\n",
      " 50 80.2M   50 40.6M    0     0   359k      0  0:03:48  0:01:55  0:01:53  369k\n",
      " 51 80.2M   51 41.0M    0     0   359k      0  0:03:48  0:01:56  0:01:52  370k\n",
      " 51 80.2M   51 41.3M    0     0   359k      0  0:03:48  0:01:57  0:01:51  370k\n",
      " 52 80.2M   52 41.7M    0     0   360k      0  0:03:48  0:01:58  0:01:50  370k\n",
      " 52 80.2M   52 42.1M    0     0   360k      0  0:03:48  0:01:59  0:01:49  367k\n",
      " 52 80.2M   52 42.4M    0     0   359k      0  0:03:48  0:02:00  0:01:48  363k\n",
      " 53 80.2M   53 42.7M    0     0   359k      0  0:03:48  0:02:01  0:01:47  365k\n",
      " 53 80.2M   53 43.1M    0     0   360k      0  0:03:48  0:02:02  0:01:46  371k\n",
      " 54 80.2M   54 43.5M    0     0   360k      0  0:03:47  0:02:03  0:01:44  366k\n",
      " 54 80.2M   54 43.8M    0     0   360k      0  0:03:48  0:02:04  0:01:44  364k\n",
      " 55 80.2M   55 44.2M    0     0   360k      0  0:03:48  0:02:05  0:01:43  368k\n",
      " 55 80.2M   55 44.6M    0     0   360k      0  0:03:47  0:02:06  0:01:41  370k\n",
      " 56 80.2M   56 44.9M    0     0   360k      0  0:03:47  0:02:07  0:01:40  367k\n",
      " 56 80.2M   56 45.3M    0     0   360k      0  0:03:47  0:02:08  0:01:39  363k\n",
      " 56 80.2M   56 45.6M    0     0   360k      0  0:03:47  0:02:09  0:01:38  369k\n",
      " 57 80.2M   57 46.0M    0     0   360k      0  0:03:47  0:02:10  0:01:37  366k\n",
      " 57 80.2M   57 46.4M    0     0   360k      0  0:03:47  0:02:11  0:01:36  367k\n",
      " 58 80.2M   58 46.6M    0     0   359k      0  0:03:48  0:02:12  0:01:36  341k\n",
      " 58 80.2M   58 46.9M    0     0   359k      0  0:03:48  0:02:13  0:01:35  332k\n",
      " 58 80.2M   58 47.2M    0     0   359k      0  0:03:48  0:02:14  0:01:34  316k\n",
      " 59 80.2M   59 47.5M    0     0   358k      0  0:03:48  0:02:15  0:01:33  313k\n",
      " 59 80.2M   59 47.8M    0     0   358k      0  0:03:49  0:02:16  0:01:33  300k\n",
      " 60 80.2M   60 48.2M    0     0   358k      0  0:03:49  0:02:17  0:01:32  320k\n",
      " 60 80.2M   60 48.5M    0     0   358k      0  0:03:49  0:02:18  0:01:31  332k\n",
      " 60 80.2M   60 48.9M    0     0   358k      0  0:03:49  0:02:19  0:01:30  341k\n",
      " 61 80.2M   61 49.2M    0     0   358k      0  0:03:49  0:02:20  0:01:29  350k\n",
      " 61 80.2M   61 49.6M    0     0   358k      0  0:03:49  0:02:21  0:01:28  363k\n",
      " 62 80.2M   62 50.0M    0     0   358k      0  0:03:48  0:02:22  0:01:26  373k\n",
      " 62 80.2M   62 50.3M    0     0   358k      0  0:03:49  0:02:23  0:01:26  367k\n",
      " 63 80.2M   63 50.7M    0     0   358k      0  0:03:48  0:02:24  0:01:24  370k\n",
      " 63 80.2M   63 51.1M    0     0   358k      0  0:03:48  0:02:25  0:01:23  370k\n",
      " 64 80.2M   64 51.4M    0     0   358k      0  0:03:48  0:02:26  0:01:22  370k\n",
      " 64 80.2M   64 51.7M    0     0   359k      0  0:03:48  0:02:27  0:01:21  364k\n",
      " 65 80.2M   65 52.1M    0     0   359k      0  0:03:48  0:02:28  0:01:20  367k\n",
      " 65 80.2M   65 52.5M    0     0   359k      0  0:03:48  0:02:29  0:01:19  367k\n",
      " 65 80.2M   65 52.9M    0     0   359k      0  0:03:48  0:02:30  0:01:18  368k\n",
      " 66 80.2M   66 53.2M    0     0   359k      0  0:03:48  0:02:31  0:01:17  374k\n",
      " 66 80.2M   66 53.5M    0     0   359k      0  0:03:48  0:02:32  0:01:16  368k\n",
      " 67 80.2M   67 53.9M    0     0   359k      0  0:03:48  0:02:33  0:01:15  371k\n",
      " 67 80.2M   67 54.3M    0     0   359k      0  0:03:48  0:02:34  0:01:14  370k\n",
      " 68 80.2M   68 54.7M    0     0   359k      0  0:03:48  0:02:35  0:01:13  370k\n",
      " 68 80.2M   68 55.1M    0     0   359k      0  0:03:48  0:02:36  0:01:12  367k\n",
      " 69 80.2M   69 55.3M    0     0   359k      0  0:03:48  0:02:37  0:01:11  361k\n",
      " 69 80.2M   69 55.7M    0     0   359k      0  0:03:48  0:02:38  0:01:10  362k\n",
      " 69 80.2M   69 56.1M    0     0   359k      0  0:03:48  0:02:39  0:01:09  362k\n",
      " 70 80.2M   70 56.4M    0     0   359k      0  0:03:48  0:02:40  0:01:08  367k\n",
      " 70 80.2M   70 56.7M    0     0   359k      0  0:03:48  0:02:41  0:01:07  355k\n",
      " 71 80.2M   71 57.1M    0     0   359k      0  0:03:48  0:02:42  0:01:06  368k\n",
      " 71 80.2M   71 57.5M    0     0   359k      0  0:03:48  0:02:43  0:01:05  368k\n",
      " 72 80.2M   72 57.9M    0     0   359k      0  0:03:48  0:02:44  0:01:04  372k\n",
      " 72 80.2M   72 58.2M    0     0   360k      0  0:03:48  0:02:45  0:01:03  366k\n",
      " 73 80.2M   73 58.6M    0     0   359k      0  0:03:48  0:02:46  0:01:02  370k\n",
      " 73 80.2M   73 58.9M    0     0   360k      0  0:03:48  0:02:47  0:01:01  370k\n",
      " 73 80.2M   73 59.3M    0     0   360k      0  0:03:48  0:02:48  0:01:00  370k\n",
      " 74 80.2M   74 59.7M    0     0   360k      0  0:03:48  0:02:49  0:00:59  370k\n",
      " 74 80.2M   74 60.0M    0     0   360k      0  0:03:48  0:02:50  0:00:58  368k\n",
      " 75 80.2M   75 60.4M    0     0   360k      0  0:03:48  0:02:51  0:00:57  368k\n",
      " 75 80.2M   75 60.7M    0     0   360k      0  0:03:48  0:02:52  0:00:56  368k\n",
      " 76 80.2M   76 61.1M    0     0   360k      0  0:03:47  0:02:53  0:00:54  368k\n",
      " 76 80.2M   76 61.5M    0     0   360k      0  0:03:47  0:02:54  0:00:53  368k\n",
      " 77 80.2M   77 61.8M    0     0   360k      0  0:03:47  0:02:55  0:00:52  365k\n",
      " 77 80.2M   77 62.2M    0     0   360k      0  0:03:47  0:02:56  0:00:51  372k\n",
      " 77 80.2M   77 62.5M    0     0   360k      0  0:03:47  0:02:57  0:00:50  369k\n",
      " 78 80.2M   78 62.9M    0     0   360k      0  0:03:47  0:02:58  0:00:49  367k\n",
      " 78 80.2M   78 63.3M    0     0   360k      0  0:03:47  0:02:59  0:00:48  367k\n",
      " 79 80.2M   79 63.6M    0     0   360k      0  0:03:47  0:03:00  0:00:47  370k\n",
      " 79 80.2M   79 64.0M    0     0   360k      0  0:03:47  0:03:01  0:00:46  366k\n",
      " 80 80.2M   80 64.3M    0     0   360k      0  0:03:47  0:03:02  0:00:45  368k\n",
      " 80 80.2M   80 64.7M    0     0   360k      0  0:03:47  0:03:03  0:00:44  370k\n",
      " 81 80.2M   81 65.1M    0     0   361k      0  0:03:47  0:03:04  0:00:43  371k\n",
      " 81 80.2M   81 65.4M    0     0   360k      0  0:03:47  0:03:05  0:00:42  367k\n",
      " 82 80.2M   82 65.8M    0     0   360k      0  0:03:47  0:03:06  0:00:41  367k\n",
      " 82 80.2M   82 66.1M    0     0   360k      0  0:03:47  0:03:07  0:00:40  368k\n",
      " 82 80.2M   82 66.5M    0     0   360k      0  0:03:47  0:03:08  0:00:39  367k\n",
      " 83 80.2M   83 66.9M    0     0   361k      0  0:03:47  0:03:09  0:00:38  368k\n",
      " 83 80.2M   83 67.2M    0     0   361k      0  0:03:47  0:03:10  0:00:37  369k\n",
      " 84 80.2M   84 67.6M    0     0   361k      0  0:03:47  0:03:11  0:00:36  370k\n",
      " 84 80.2M   84 68.0M    0     0   361k      0  0:03:47  0:03:12  0:00:35  370k\n",
      " 85 80.2M   85 68.3M    0     0   361k      0  0:03:47  0:03:13  0:00:34  375k\n",
      " 85 80.2M   85 68.7M    0     0   361k      0  0:03:47  0:03:14  0:00:33  370k\n",
      " 86 80.2M   86 69.0M    0     0   361k      0  0:03:47  0:03:15  0:00:32  371k\n",
      " 86 80.2M   86 69.4M    0     0   361k      0  0:03:47  0:03:16  0:00:31  372k\n",
      " 86 80.2M   86 69.7M    0     0   361k      0  0:03:47  0:03:17  0:00:30  369k\n",
      " 87 80.2M   87 70.1M    0     0   361k      0  0:03:47  0:03:18  0:00:29  368k\n",
      " 87 80.2M   87 70.5M    0     0   361k      0  0:03:47  0:03:19  0:00:28  363k\n",
      " 88 80.2M   88 70.8M    0     0   361k      0  0:03:47  0:03:20  0:00:27  366k\n",
      " 88 80.2M   88 71.2M    0     0   361k      0  0:03:47  0:03:21  0:00:26  367k\n",
      " 89 80.2M   89 71.6M    0     0   361k      0  0:03:47  0:03:22  0:00:25  370k\n",
      " 89 80.2M   89 71.9M    0     0   361k      0  0:03:47  0:03:23  0:00:24  368k\n",
      " 90 80.2M   90 72.2M    0     0   361k      0  0:03:47  0:03:24  0:00:23  367k\n",
      " 90 80.2M   90 72.6M    0     0   361k      0  0:03:47  0:03:25  0:00:22  373k\n",
      " 91 80.2M   91 73.0M    0     0   361k      0  0:03:47  0:03:26  0:00:21  367k\n",
      " 91 80.2M   91 73.4M    0     0   361k      0  0:03:47  0:03:27  0:00:20  372k\n",
      " 91 80.2M   91 73.7M    0     0   361k      0  0:03:46  0:03:28  0:00:18  371k\n",
      " 92 80.2M   92 74.1M    0     0   361k      0  0:03:47  0:03:29  0:00:18  369k\n",
      " 92 80.2M   92 74.4M    0     0   361k      0  0:03:47  0:03:30  0:00:17  363k\n",
      " 93 80.2M   93 74.8M    0     0   361k      0  0:03:47  0:03:31  0:00:16  367k\n",
      " 93 80.2M   93 75.2M    0     0   361k      0  0:03:46  0:03:32  0:00:14  367k\n",
      " 94 80.2M   94 75.5M    0     0   362k      0  0:03:46  0:03:33  0:00:13  365k\n",
      " 94 80.2M   94 75.9M    0     0   361k      0  0:03:46  0:03:34  0:00:12  367k\n",
      " 95 80.2M   95 76.2M    0     0   362k      0  0:03:46  0:03:35  0:00:11  371k\n",
      " 95 80.2M   95 76.6M    0     0   361k      0  0:03:47  0:03:36  0:00:11  360k\n",
      " 95 80.2M   95 76.9M    0     0   361k      0  0:03:47  0:03:37  0:00:10  350k\n",
      " 96 80.2M   96 77.3M    0     0   361k      0  0:03:47  0:03:38  0:00:09  351k\n",
      " 96 80.2M   96 77.6M    0     0   361k      0  0:03:47  0:03:39  0:00:08  354k\n",
      " 97 80.2M   97 78.0M    0     0   361k      0  0:03:47  0:03:40  0:00:07  353k\n",
      " 97 80.2M   97 78.3M    0     0   361k      0  0:03:47  0:03:41  0:00:06  364k\n",
      " 98 80.2M   98 78.7M    0     0   362k      0  0:03:46  0:03:42  0:00:04  377k\n",
      " 98 80.2M   98 79.1M    0     0   362k      0  0:03:46  0:03:43  0:00:03  380k\n",
      " 99 80.2M   99 79.4M    0     0   362k      0  0:03:46  0:03:44  0:00:02  371k\n",
      " 99 80.2M   99 79.8M    0     0   361k      0  0:03:46  0:03:45  0:00:01  368k\n",
      " 99 80.2M   99 80.1M    0     0   362k      0  0:03:46  0:03:46 --:--:--  367k\n",
      "100 80.2M  100 80.2M    0     0   362k      0  0:03:46  0:03:46 --:--:--  369k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRaCba_y2tdx",
    "outputId": "c322bb30-9311-43a6-d01c-48b25aa187d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Qzw_3pUi2xWc"
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgLd8rLs26q0",
    "outputId": "ddbc5675-e21a-4a4a-e573-bf36a48b7600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQpLvNg628W5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "UczenieMaszynoweLab3_text_generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
